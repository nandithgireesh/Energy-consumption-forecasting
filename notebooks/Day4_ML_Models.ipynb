{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# âš¡ Day 4 â€” Classical ML Models\n",
                "## Energy Consumption Forecasting | Claysys AI Hackathon 2026\n",
                "\n",
                "**Date:** February 22, 2026  \n",
                "**Objective:** Train Random Forest, XGBoost, and LightGBM regressors using engineered lag features.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '..')\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "from src.models.ml_models import RandomForestForecaster, XGBoostForecaster, LightGBMForecaster\n",
                "from src.evaluation import compute_metrics, plot_predictions, compare_models, plot_model_comparison\n",
                "\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "plt.rcParams.update({'figure.dpi': 120})\n",
                "print('âœ… Day 4 Setup complete')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Processed Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_df = pd.read_csv('../data/processed/train.csv', index_col='Datetime', parse_dates=True)\n",
                "test_df  = pd.read_csv('../data/processed/test.csv',  index_col='Datetime', parse_dates=True)\n",
                "\n",
                "# Remove non-numeric and non-feature columns\n",
                "drop_cols = ['season']\n",
                "train_df = train_df.drop(columns=[c for c in drop_cols if c in train_df.columns])\n",
                "test_df  = test_df.drop(columns=[c for c in drop_cols if c in test_df.columns])\n",
                "\n",
                "TARGET = 'Global_active_power'\n",
                "\n",
                "# Features = everything except target\n",
                "feature_cols = [c for c in train_df.select_dtypes(include=[np.number]).columns if c != TARGET]\n",
                "\n",
                "X_train, y_train = train_df[feature_cols], train_df[TARGET]\n",
                "X_test,  y_test  = test_df[feature_cols],  test_df[TARGET]\n",
                "\n",
                "print(f'X_train: {X_train.shape},  y_train: {y_train.shape}')\n",
                "print(f'X_test : {X_test.shape },  y_test : {y_test.shape }')\n",
                "print(f'Number of features: {X_train.shape[1]}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Model 1 â€” Random Forest"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rf_model = RandomForestForecaster(n_estimators=200, max_depth=None)\n",
                "rf_model.fit(X_train, y_train)\n",
                "\n",
                "rf_preds = rf_model.predict(X_test)\n",
                "metrics_rf = compute_metrics(y_test.values, rf_preds, model_name='Random Forest')\n",
                "\n",
                "# Feature importance\n",
                "fi = rf_model.feature_importance().head(15)\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "ax.barh(fi['feature'][::-1], fi['importance'][::-1], color='steelblue', alpha=0.85)\n",
                "ax.set_title('Random Forest â€” Top 15 Feature Importances', fontweight='bold')\n",
                "ax.set_xlabel('Importance')\n",
                "plt.tight_layout()\n",
                "plt.savefig('../reports/figures/rf_feature_importance.png', bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "rf_model.save('random_forest.pkl')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_predictions(y_test.values[:168], rf_preds[:168],\n",
                "                 index=test_df.index[:168],\n",
                "                 model_name='Random Forest (First Week of Test)',\n",
                "                 filename='rf_predictions.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model 2 â€” XGBoost"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Validation split for early stopping (last 10% of train)\n",
                "val_size = int(len(X_train) * 0.1)\n",
                "X_tr, X_val = X_train.iloc[:-val_size], X_train.iloc[-val_size:]\n",
                "y_tr, y_val = y_train.iloc[:-val_size], y_train.iloc[-val_size:]\n",
                "\n",
                "xgb_model = XGBoostForecaster(n_estimators=500, learning_rate=0.05, max_depth=6)\n",
                "xgb_model.fit(X_tr, y_tr, X_val=X_val, y_val=y_val)\n",
                "\n",
                "xgb_preds = xgb_model.predict(X_test)\n",
                "metrics_xgb = compute_metrics(y_test.values, xgb_preds, model_name='XGBoost')\n",
                "\n",
                "xgb_model.save('xgboost.pkl')\n",
                "\n",
                "plot_predictions(y_test.values[:168], xgb_preds[:168],\n",
                "                 index=test_df.index[:168],\n",
                "                 model_name='XGBoost (First Week of Test)',\n",
                "                 filename='xgb_predictions.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model 3 â€” LightGBM"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lgbm_model = LightGBMForecaster(n_estimators=500, learning_rate=0.05, num_leaves=63)\n",
                "lgbm_model.fit(X_tr, y_tr, X_val=X_val, y_val=y_val)\n",
                "\n",
                "lgbm_preds = lgbm_model.predict(X_test)\n",
                "metrics_lgbm = compute_metrics(y_test.values, lgbm_preds, model_name='LightGBM')\n",
                "\n",
                "lgbm_model.save('lightgbm.pkl')\n",
                "\n",
                "plot_predictions(y_test.values[:168], lgbm_preds[:168],\n",
                "                 index=test_df.index[:168],\n",
                "                 model_name='LightGBM (First Week of Test)',\n",
                "                 filename='lgbm_predictions.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Day 4 Model Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results_day4 = [metrics_rf, metrics_xgb, metrics_lgbm]\n",
                "comparison_df = compare_models(results_day4)\n",
                "\n",
                "comparison_df.to_csv('../reports/ml_results.csv')\n",
                "plot_model_comparison(comparison_df, metric='RMSE')\n",
                "\n",
                "# Plot all models side-by-side on a test week\n",
                "fig, ax = plt.subplots(figsize=(15, 5))\n",
                "n = 168  # 1 week\n",
                "ax.plot(test_df.index[:n], y_test.values[:n], label='Actual', color='black', linewidth=1.5, zorder=5)\n",
                "ax.plot(test_df.index[:n], rf_preds[:n],   label='Random Forest', color='#2196F3', linewidth=1, linestyle='--')\n",
                "ax.plot(test_df.index[:n], xgb_preds[:n],  label='XGBoost',       color='#FF9800', linewidth=1, linestyle='--')\n",
                "ax.plot(test_df.index[:n], lgbm_preds[:n], label='LightGBM',      color='#4CAF50', linewidth=1, linestyle='--')\n",
                "ax.set_title('ML Models â€” 1 Week Forecast Comparison', fontweight='bold')\n",
                "ax.set_ylabel('Global Active Power (kW)')\n",
                "ax.legend(loc='upper right')\n",
                "plt.tight_layout()\n",
                "plt.savefig('../reports/figures/ml_comparison_week.png', bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print('\\nðŸŽ‰ Day 4 Complete! ML models trained and saved.')\n",
                "print('   Ready for Day 5: Deep Learning â€” LSTM & GRU with PyTorch')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}